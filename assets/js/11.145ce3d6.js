(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{367:function(e,t,o){"use strict";o.r(t);var n=o(44),i=Object(n.a)({},(function(){var e=this,t=e.$createElement,o=e._self._c||t;return o("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[o("h2",{attrs:{id:"overview"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),o("p",[e._v("This chapter introduces the SDK which contains the library of resources necessary for the application and the related basic application routines.")]),e._v(" "),o("h2",{attrs:{id:"sdk-structure"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#sdk-structure"}},[e._v("#")]),e._v(" SDK Structure")]),e._v(" "),o("table",[o("thead",[o("tr",[o("th",[e._v("Directory")]),e._v(" "),o("th",[e._v("Content")])])]),e._v(" "),o("tbody",[o("tr",[o("td",[e._v("./Platform")]),e._v(" "),o("td",[e._v("Contains  scripts for generating runtime environments for different platforms.")])]),e._v(" "),o("tr",[o("td",[e._v("./SDK/docs")]),e._v(" "),o("td",[e._v("Contains  SDK-related introduction and documentation.")])]),e._v(" "),o("tr",[o("td",[e._v("./SDK/Drivers")]),e._v(" "),o("td",[e._v("Contains  the drivers that must be installed for different platforms.")])]),e._v(" "),o("tr",[o("td",[e._v("./SDK/Example")]),e._v(" "),o("td",[e._v("Contains  SDK related routines.")])]),e._v(" "),o("tr",[o("td",[e._v("./SDK/Source")]),e._v(" "),o("td",[e._v("Includes  firmware, model and SDK library files.")])]),e._v(" "),o("tr",[o("td",[e._v("./SDK/Tools")]),e._v(" "),o("td",[e._v("Includes  relevant model conversion and compilation tools.")])]),e._v(" "),o("tr",[o("td",[e._v("./Viewer")]),e._v(" "),o("td",[e._v("Includes  compiled Viewer and QT source code.")])])])]),e._v(" "),o("h2",{attrs:{id:"supported-platform"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#supported-platform"}},[e._v("#")]),e._v(" Supported Platform")]),e._v(" "),o("p",[e._v("SDK支持的产品如下：")]),e._v(" "),o("ul",[o("li",[o("p",[e._v("OpenNCC DK")])]),e._v(" "),o("li",[o("p",[e._v("OpenNCC Lite")])]),e._v(" "),o("li",[o("p",[e._v("OpenNCC USB")])])]),e._v(" "),o("p",[e._v("SDK支持的平台如下：")]),e._v(" "),o("ul",[o("li",[o("p",[e._v("Linux")]),e._v(" "),o("ul",[o("li",[o("p",[e._v("Ubuntu 16.04, Ubuntu 18.04")])]),e._v(" "),o("li",[o("p",[e._v("Raspberry Pi")])]),e._v(" "),o("li",[o("p",[e._v("NVIDIA")])])])]),e._v(" "),o("li",[o("p",[e._v("Windows 10")])])]),e._v(" "),o("p",[e._v("SDK支持的语言如下：")]),e._v(" "),o("ul",[o("li",[e._v("C/C++")]),e._v(" "),o("li",[e._v("Python3.5、Python3.7")])]),e._v(" "),o("h3",{attrs:{id:"sdk-development-package-directory-structure"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#sdk-development-package-directory-structure"}},[e._v("#")]),e._v(" SDK Development Package Directory Structure")]),e._v(" "),o("table",[o("thead",[o("tr",[o("th",[e._v("Directory")]),e._v(" "),o("th",[e._v("Contents")])])]),e._v(" "),o("tbody",[o("tr",[o("td",[e._v("Example/How_to/... /How_to_use_sdk")]),e._v(" "),o("td",[e._v("Sample  application, how to use the SDK library in your project.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/How_to/... /Capture_video")]),e._v(" "),o("td",[e._v("Sample  application, how to get a video stream using the SDK library.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/How_to/... /Load_a_model")]),e._v(" "),o("td",[e._v("Sample  application, how to download a deep learning model in Blob format using the  SDK library.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/How_to/... /work_with_multiple_models")]),e._v(" "),o("td",[e._v("Sample  application, how to use a second-level model.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/How_to/... /python3")]),e._v(" "),o("td",[e._v("Python  related examples.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/How_to/OpenNCC-EMMC")]),e._v(" "),o("td",[e._v("Sample application,how to develop on OpenNCC-eMMC.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/Linkage_demo/  "),o("br"),e._v("work with AlwaysAI /  "),o("br"),e._v("pedestrian_tracking_demo")]),e._v(" "),o("td",[e._v("Face  model, using AlwaysAI to parse the result display and count the number of  people passing the recognition area.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/Linkage_demo/  "),o("br"),e._v("work_with_OpenVINO/  "),o("br"),e._v("human_pose_estimation_demo")]),e._v(" "),o("td",[e._v("Human  skeleton model, using OpenVINO to parse the result display.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/Linkage_demo/  "),o("br"),e._v("work_with_OpenVINO/  "),o("br"),e._v("interactive_face_detection_demo")]),e._v(" "),o("td",[e._v("Face,  age, gender, and mood models, using OpenVINO parsing results.")])]),e._v(" "),o("tr",[o("td",[e._v("Example/Linkage_demo/  "),o("br"),e._v("work_with_PaddlePaddle")]),e._v(" "),o("td",[e._v("OCR Sample application, include a network link to the OCR warehouse.")])]),e._v(" "),o("tr",[o("td",[e._v("Tools/myriad_compiler")]),e._v(" "),o("td",[e._v("IR file conversion Blob file tool")])]),e._v(" "),o("tr",[o("td",[e._v("Tools/deployment")]),e._v(" "),o("td",[e._v("Kit deployment script")])])])]),e._v(" "),o("h2",{attrs:{id:"openvino-installation-and-getting-start"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#openvino-installation-and-getting-start"}},[e._v("#")]),e._v(" OpenVINO installation and getting start")]),e._v(" "),o("p",[e._v("   To deploy a deep learning model on end-point target devices, you need to optimize and convert a trained model to the VPU characteristics to achieve higher operating performance. OpenNCC is compatible with OpenVINO's tool set and model format, and needs to rely on Intel OpenVINO's model optimizer to complete model optimization and conversion into Blob format. When using OpenNCC SDK, you need to install OpenVINO as follows:\nIf you need to convert the trained model yourself, you need to install OpenVINO to run the model optimizer.\nWhen OpenVINO runs in a mixed mode with the OpenVINO inference engine, it also needs OpenVINO support.")]),e._v(" "),o("h3",{attrs:{id:"download-and-install-openvino"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#download-and-install-openvino"}},[e._v("#")]),e._v(" Download and Install OpenVINO")]),e._v(" "),o("p",[e._v("  OpenNCC currently supports OpenVINO version: 2020.3.194, OpenVINO installation reference "),o("RouterLink",{attrs:{to:"/7_openvino_install.html"}},[e._v("here")]),e._v(".")],1),e._v(" "),o("h3",{attrs:{id:"intel-free-model-download"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#intel-free-model-download"}},[e._v("#")]),e._v(" Intel Free Model Download")]),e._v(" "),o("p",[e._v("  OpenNCC supports OpenVINO models, Intel has a large number of free trained models for learning reference and testing. After we have installed OpenVINO, we can use the Intel download tool to download the model collection. Model download tool path: "),o("code",[e._v("openvino/deployment_tools/tools/model_downloader/downloader.py")]),e._v(", common commands are as follows:")]),e._v(" "),o("ul",[o("li",[e._v("View all downloadable models：./downloader.py --print")]),e._v(" "),o("li",[e._v("Download the specified model：./downloader.py --name *")])]),e._v(" "),o("p",[e._v("For example, download a face detection model ："),o("code",[e._v("./downloader.py --name face-detection-adas-0001-fp16")]),o("br"),e._v(" "),o("img",{attrs:{src:"/openncc/docimg/sw_figure1.png",alt:"Figure-1"}})]),e._v(" "),o("h3",{attrs:{id:"model-optimization-and-format-conversion"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#model-optimization-and-format-conversion"}},[e._v("#")]),e._v(" Model Optimization and Format Conversion")]),e._v(" "),o("p",[e._v(" When we need to deploy a trained model to OpenNCC, we need to optimize and transform the model. After installing OpenVINO, you can use the model optimization tool: "),o("code",[e._v("/opt/intel/openvino/deployment_tools/model_optimizer/mo.py")]),e._v(" to optimize the model. For specific documents, see the official Intel documents: "),o("a",{attrs:{href:"https://docs.openvinotoolkit.org/2020.3/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Model Optimizer Developer Guide"),o("OutboundLink")],1),e._v(".")]),e._v(" "),o("p",[e._v(" After the model optimization is completed, the model needs to be converted to the Blob format before it can be deployed on OpenNCC. In the OpenVINO installation directory: "),o("code",[e._v("/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64myriad_compile")]),e._v(" tool, the method of use is as follows:\nEnter from the command line terminal："),o("code",[e._v("./myriad_compile -m input_xxx-fp16.xml  -o output_xxx.blob  -VPU_PLATFORM VPU_2480 -VPU_NUMBER_OF_SHAVES  8  -VPU_NUMBER_OF_CMX_SLICES 8")])]),e._v(" "),o("p",[e._v("  After the format conversion is completed, the model can be deployed on OpenNCC, refer to: "),o("code",[e._v("ncc_SDK/Samples/How_to/load a model")]),e._v(", or use the OpenNCC View interface program to add the model to deploy and test it.")]),e._v(" "),o("h2",{attrs:{id:"openncc-operating-mechanism"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#openncc-operating-mechanism"}},[e._v("#")]),e._v(" OpenNCC Operating Mechanism")]),e._v(" "),o("p",[e._v("  From a model training environment to embedded deployment, it is a very important task, which requires mastering the framework of deep learning, such as commonly used: Caffe*, TensorFlow*, MXNet*, Kaldi*, etc.In addition, it is very important to master the deployed embedded platform. You need to understand the platform performance, system architecture characteristics, and then combine the platform characteristics to optimize the training model framework, and finally tune, transplant, and deploy to the embedded platform.")]),e._v(" "),o("p",[e._v("  OpenNCC focuses on the rapid deployment of deep learning models, is compatible with Intel OpenVINO tools, and for embedded graphics and image application scenarios, it has completed the integration of different resolution sensors from 2MP to 20MP on end-point target devices, and the end-point target devices has realized the deployment of professional-level ISP.  OpenVINO optimized converted model files can be dynamically downloaded to the end-point OpenNCC camera to achieve rapid deployment of deep learning models. OpenNCC has designed independent working mode, mixed development mode and co-processing compute stick mode to adapt to different work application scenarios.")]),e._v(" "),o("h3",{attrs:{id:"openncc-standalone-mode"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#openncc-standalone-mode"}},[e._v("#")]),e._v(" OpenNCC Standalone Mode")]),e._v(" "),o("p",[e._v(" In the independent mode, OpenNCC independently runs a deep learning model, and feeds back the inference results to the user through the OpenNCC SDK API.\nThe application deployment process is as follows:")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/sw_figure2.png",alt:"Figure-2"}})]),e._v(" "),o("p",[e._v(" According to the OpenVINO documentation, for a specific training framework "),o("a",{attrs:{href:"https://docs.openvinotoolkit.org/2020.3/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Configure Model Optimizer"),o("OutboundLink")],1)]),e._v(" "),o("p",[e._v(" Run "),o("a",{attrs:{href:"https://docs.openvinotoolkit.org/2020.3/_docs_IE_DG_Introduction.html#MO",target:"_blank",rel:"noopener noreferrer"}},[e._v("Model Optimizer"),o("OutboundLink")],1),e._v(" to produce an optimized Intermediate Representation (IR) of the model based on the trained network topology, weights and biases values, and other optional parameters.\nThe IR is a pair of files that describe the whole model:")]),e._v(" "),o("ul",[o("li",[e._v(".xml: The topology file - an XML file that describes the network topology")]),e._v(" "),o("li",[e._v(".bin: The trained data file - a .bin file that contains the weights and biases binary data")]),e._v(" "),o("li",[e._v("Then run myriad_compile to generate a BLOB file from the IR file.")]),e._v(" "),o("li",[e._v("To integrate the BLOB model file generated after optimization using OpenNCC SDK, see the demo program of "),o("code",[e._v("Examples/How_to/Load a model")]),e._v(" under SDK.")]),e._v(" "),o("li",[e._v("  OpenNCC View is an application demonstration program with an operating interface integrated with OpenNCC SDK. You can also use OpenView to deploy models and obtain test results. Refer to OpenNCC View Guide Because different depth models have differentiated inference output results, if users cannot find a suitable post-processing analytical model under the SDK, they need to refer to "),o("code",[e._v("Examples/How_to/load a model")]),e._v(" and write post-processing code in combination with their own application scenarios.")])]),e._v(" "),o("h4",{attrs:{id:"secondary-model-operation-support"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#secondary-model-operation-support"}},[e._v("#")]),e._v(" Secondary Model Operation Support")]),e._v(" "),o("p",[e._v("Considering the end-to-side computing capability, at present, SDK multi-level models support cascading of two-level models, as shown in the following figure:")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/ch/SoftManualF10.jpg",alt:"F"}})]),e._v(" "),o("p",[e._v("The first level model must be a target detection or classification model, and the output is defined as follows:")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/ch/SoftManualF11.jpg",alt:"F"}})]),e._v(" "),o("p",[e._v("process：")]),e._v(" "),o("p",[e._v("1）After pre CV [1], the original image scale is converted to the input size of the first level model, and the corresponding format conversion is performed. Then the first level model reasoning calculation is performed, and the first level reasoning result is output to pre CV [2].")]),e._v(" "),o("ol",{attrs:{start:"2"}},[o("li",[e._v("The pre CV [2] module analyzes the reasoning results of the first level model, and takes the qualified label and conf detection target according to the coordinate starting point (x)_ min, y_ Min), the end point (x)_ max,y_ Max) from the original graph's Cross and scale are converted to the input size of the secondary model, and the corresponding format conversion is performed to enter the second level model reasoning.")])]),e._v(" "),o("p",[e._v("3）Finally, the reasoning results of the first level model and all the second level models are packaged and output together.")]),e._v(" "),o("p",[e._v("Model output analysis (parameter configuration in the figure is: valid label: 2,3, conf = 0.8)")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/ch/SoftManualF12.png",alt:"F"}})]),e._v(" "),o("p",[e._v("Example："),o("code",[e._v("Examples/How_to/work_with_multiple_models")]),e._v(",the first level model is vehicle and license plate detection, the second level model is license plate detection, and the effective label is set to 2")]),e._v(" "),o("p",[e._v("Based on the detection results of the first stage, the detection coordinates of the first stage are adjusted appropriately, which is conducive to the identification of:")]),e._v(" "),o("p",[e._v("*Fine tuning the starting point to the left and up（startXAdj，startYAdj ）")]),e._v(" "),o("p",[e._v("*Bottom right down fine adjustment（endXAdj，endYAdj）")]),e._v(" "),o("p",[e._v("cnn2PrmSet.startXAdj  = -5;")]),e._v(" "),o("p",[e._v("cnn2PrmSet.startYAdj  = -5;")]),e._v(" "),o("p",[e._v("cnn2PrmSet.endXAdj   = 5;")]),e._v(" "),o("p",[e._v("cnn2PrmSet.endYAdj   = 5;")]),e._v(" "),o("h3",{attrs:{id:"openncc-mixed-mode"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#openncc-mixed-mode"}},[e._v("#")]),e._v(" OpenNCC Mixed Mode")]),e._v(" "),o("p",[e._v(" When it is necessary to solve some complex application scenarios, multiple network model combination processing is required, OpenNCC end-point computing performance cannot be met, or the end-side processing needs to be concentrated on the edge side for post-processing, system expansion is often required. Run the models with high real-time requirements on the OpenNCC end-point, and the other models on the post-processing edge machine or cloud.")]),e._v(" "),o("p",[e._v(" As shown in the figure, Model-1 runs on the OpenNCC end-point  to complete the pre-processing of the video stream. OpenNCC returns the results of the first-level processing model to the user application. Model-1 and Model-2 fully run under the OpenVINO inference engine to implement subsequent processing.")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/sw_figure3.png",alt:"Figure-3"}})]),e._v(" "),o("p",[e._v(" In "),o("code",[e._v("Examples/Linkage_demo/work_with_OpenVINO")]),e._v(" demonstrated how to combine OpenNCC and OpenVINO on Host PC to implement a distributed AI system.")]),e._v(" "),o("h3",{attrs:{id:"co-processing-compute-stick-mode"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#co-processing-compute-stick-mode"}},[e._v("#")]),e._v(" Co-processing Compute Stick Mode")]),e._v(" "),o("p",[e._v(" OpenNCC's co-processing mode is similar to Intel NCS2. In this mode of operation, OpenNCC's vision sensor does not work, and users can use OpenNCC alone to achieve full compatibility with the OpenVINO environment. The typical deep learning model deployment process of OpenVINO is as follows:  S")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/sw_figure4.png",alt:"Figure-4"}})]),e._v(" "),o("p",[o("a",{attrs:{href:"https://docs.openvinotoolkit.org/2020.3/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Configure Model Optimizer"),o("OutboundLink")],1),e._v(" for specific training framework according to OpenVINO documentation.")]),e._v(" "),o("p",[e._v(" Run Model Optimizer to produce an optimized Intermediate Representation (IR) of the model based on the trained network topology, weights and biases values, and other optional parameters.")]),e._v(" "),o("p",[e._v(" Download the optimized IR file to OpenNCC to run the Inference Engine. For details, refer to OpenVINO documents: "),o("a",{attrs:{href:"https://docs.openvinotoolkit.org/2020.3/_docs_IE_DG_Introduction.html#IE",target:"_blank",rel:"noopener noreferrer"}},[e._v("Inference Engine validation application"),o("OutboundLink")],1),e._v(" and "),o("a",{attrs:{href:"https://docs.openvinotoolkit.org/2019_R1.1/_docs_IE_DG_Samples_Overview.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("sample applications."),o("OutboundLink")],1)]),e._v(" "),o("p",[e._v(" Copy "),o("code",[e._v("Source/Firmware/fw /usb-ma2x8x.mvcmd")]),e._v(" and replace openvino/inference_engine/lib/intel64/usb-ma2x8x.mvcmd in the openvino installation directory.(Remarks: usb-ma2x8x.mvcmd in the openvino installation directory must be backed up before replacement. This file needs to be restored when using NCS2 inference)")]),e._v(" "),o("h3",{attrs:{id:"difference-between-independent-mode-and-co-processing-mode"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#difference-between-independent-mode-and-co-processing-mode"}},[e._v("#")]),e._v(" Difference between Independent Mode and Co-processing Mode")]),e._v(" "),o("p",[e._v(" The right side of the figure below is the independent mode of OpenNCC, and the left side is the co-processing mode of OpenNCC (similar to Intel NCS2).")]),e._v(" "),o("p",[o("img",{attrs:{src:"/openncc/docimg/sw_figure5.png",alt:"Figure-5"}})]),e._v(" "),o("p",[e._v(" When we need to deploy a vision-based deep learning model, first we need to obtain a high-quality video stream, then run the inference engine to calculate the input image data, and finally output the result. For the co-processing mode on the left, we need an OpenNCC or Intel NCS2 implements end-to-side reasoning. At the same time, we need to obtain a video stream from a camera and send the video frame to OpenNCC  via USB. In the independent mode on the right, no additional camera is needed to obtain the video stream. We only need to download the model to OpenNCC to obtain the deduction results.")]),e._v(" "),o("p",[e._v("Refer to OpenVINO official website:https://docs.openvinotoolkit.org/")])])}),[],!1,null,null,null);t.default=i.exports}}]);